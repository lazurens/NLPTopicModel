{
    "collab_server" : "",
    "contents" : "library(tidyverse)\nlibrary(tidytext)\nlibrary(tm)\nlibrary(stringr)\nlibrary(SnowballC)\nlibrary(topicmodels)\nlibrary(dplyr)\nlibrary(stringi)\n\nif (!file.exists(\"data-raw/reviews\")) {\n  tmp <- tempfile(fileext = \".tar.gz\")\n  download.file(\"http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\", \n                tmp, quiet = TRUE)\n  untar(tmp, exdir = \"data-raw/reviews\")\n  unlink(tmp)\n}\n\npath <- file.path(\"data-raw\", \"reviews\", \"txt_sentoken\")\npos <- list.files(file.path(path, \"pos\"))\nneg <- list.files(file.path(path, \"neg\"))\npos.files <- file.path(path, \"pos\", pos)\nneg.files <- file.path(path, \"neg\", neg)\nall.files <- c(pos.files, neg.files)\ntxt <- lapply(all.files, readLines)\nnms <- gsub(\"data-raw/reviews/txt_sentoken\", \"\", all.files)\nreviews <- setNames(txt, nms)\nreviews <- sapply(reviews, function(x) paste(x, collapse = \" \"))\n\nsave(reviews, file = \"data/reviews.rdata\", compress = \"xz\")\n\n#posts_tbl <- readRDS(file = \"20170302_TopicsFromTwitter/data/posts.Rds\")\n# Only keep popular tweets\n#posts_tbl <- filter(posts_tbl, nb_actions >= 1)\n#reviews <- posts_tbl\n# read in some stopwords:\nlibrary(tm)\nstop_words <- stopwords(\"SMART\")\n\n# pre-processing:\nreviews <- gsub(\"'\", \"\", reviews)  # remove apostrophes\nreviews <- gsub(\"[[:punct:]]\", \" \", reviews)  # replace punctuation with space\nreviews <- gsub(\"[[:cntrl:]]\", \" \", reviews)  # replace control characters with space\nreviews <- gsub(\"^[[:space:]]+\", \"\", reviews) # remove whitespace at beginning of documents\nreviews <- gsub(\"[[:space:]]+$\", \"\", reviews) # remove whitespace at end of documents\nreviews <- tolower(reviews)  # force to lowercase\n# tokenize on space and output as a list:\ndoc.list <- strsplit(creviews, \"[[:space:]]+\")\ndoc.list <- strsplit(reviews, \"[[:space:]]+\")\n\n# compute the table of terms:\nterm.table <- table(unlist(doc.list))\nterm.table <- sort(term.table, decreasing = TRUE)\n\n# remove terms that are stop words or occur fewer than 5 times:\ndel <- names(term.table) %in% stop_words | term.table < 5\nterm.table <- term.table[!del]\nvocab <- names(term.table)\n\n# now put the documents into the format required by the lda package:\nget.terms <- function(x) {\n  index <- match(x, vocab)\n  index <- index[!is.na(index)]\n  rbind(as.integer(index - 1), as.integer(rep(1, length(index))))\n}\ndocuments <- lapply(doc.list, get.terms)\n######################\n# Compute some statistics related to the data set:\nD <- length(documents)  # number of documents (2,000)\nW <- length(vocab)  # number of terms in the vocab (14,568)\ndoc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document [312, 288, 170, 436, 291, ...]\nN <- sum(doc.length)  # total number of tokens in the data (546,827)\nterm.frequency <- as.integer(term.table)  # frequencies of terms in the corpus [8939, 5544, 2411, 2410, 2143, ...]\n\n# MCMC and model tuning parameters:\nK <- 20\nG <- 5000\nalpha <- 0.02\neta <- 0.02\n\n# Fit the model:\nlibrary(lda)\nset.seed(357)\nt1 <- Sys.time()\nfit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab, \n                                   num.iterations = G, alpha = alpha, \n                                   eta = eta, initial = NULL, burnin = 0,\n                                   compute.log.likelihood = TRUE)\nt2 <- Sys.time()\nt2 - t1  # about 21 minutes on laptop\n\ntheta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))\nphi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))\n\nMovieReviews <- list(phi = phi,\n                     theta = theta,\n                     doc.length = doc.length,\n                     vocab = vocab,\n                     term.frequency = term.frequency)\nlibrary(LDAvis)\n\n# create the JSON object to feed the visualization:\njson <- createJSON(phi = MovieReviews$phi, \n                   theta = MovieReviews$theta, \n                   doc.length = MovieReviews$doc.length, \n                   vocab = MovieReviews$vocab, \n                   term.frequency = MovieReviews$term.frequency)\n\nserVis(json, out.dir = 'vis', open.browser = TRUE)\n",
    "created" : 1496330662542.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2025662274",
    "id" : "7B72210C",
    "lastKnownWriteTime" : 1496333714,
    "last_content_update" : 1496333714792,
    "path" : "~/GitHub/discursus_io_analysis/20170404_TopicExtraction_Algorithm/lda/maintweets3.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}